{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "3e00ae03-c0fa-4f5b-8d29-ac3f316d5e84",
    "_uuid": "3181182e-05ee-49fe-80bf-dca3dacabe15"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "294c3404-ffa2-498c-85dc-b88fef3723dd",
    "_uuid": "8c8ffe3c-91cf-424f-94fc-2595e2d4d4d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  grpc://10.0.0.2:8470\n",
      "REPLICAS: 8\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "\n",
    "AUTO     = tf.data.experimental.AUTOTUNE\n",
    "REPLICAS = strategy.num_replicas_in_sync\n",
    "#REPLICAS = 8\n",
    "print(f'REPLICAS: {REPLICAS}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "9c0a194c-75d8-45c5-87b9-f2966e4fec13",
    "_uuid": "4e9583c6-3da2-4885-8a2b-2774bf6b0152"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/train.csv\")\n",
    "df_test = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/test.csv\")\n",
    "sample_sub = pd.read_csv(\"/kaggle/input/siim-isic-melanoma-classification/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "72247b6f-5ffa-4c6e-9a90-68f1fff993aa",
    "_uuid": "0243c269-d6f0-48ac-b9f4-c08e954cc2ee"
   },
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "3aabc096-7b13-48cd-955f-d5a6180d3d25",
    "_uuid": "17fee8a8-abca-4238-a32d-4e858aff1d0d"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from kaggle_datasets import KaggleDatasets\n",
    "GCS_PATH2 = KaggleDatasets().get_gcs_path(\"isic2019-512x512\")\n",
    "GCS_PATH1 = KaggleDatasets().get_gcs_path(\"melanoma-512x512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "eadf5f02-4a06-464e-9d99-9aa295aa9e22",
    "_uuid": "76438a85-3b84-4ef6-9374-46b73741b79a"
   },
   "outputs": [],
   "source": [
    "train_filenames1 = tf.io.gfile.glob(GCS_PATH1 + '/train*.tfrec')\n",
    "train_filenames2 = tf.io.gfile.glob(GCS_PATH2 + '/train*.tfrec')\n",
    "test_filenames = tf.io.gfile.glob(GCS_PATH1 + '/test*.tfrec')\n",
    "data_filenames = train_filenames1 + train_filenames2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "17034159-df49-4205-baf5-13b9cf093448",
    "_uuid": "ebb63a99-2a13-4ec3-a1a0-cdda1cb4bd19"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_filenames, valid_filenames = train_test_split(data_filenames, test_size=0.2, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "33827990-faf5-42a7-827d-3f403a1ee78d",
    "_uuid": "fcaf0c47-6b2f-45be-82bd-da794579e4ed"
   },
   "outputs": [],
   "source": [
    "def read_labeled_tfrecord(example):\n",
    "    LABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"target\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means single element\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n",
    "    image = example['image']\n",
    "    label = example['target']\n",
    "    return image, label # returns a dataset of (image, label) pairs\n",
    "\n",
    "def read_unlabeled_tfrecord(example, return_image_name=True):\n",
    "    UNLABELED_TFREC_FORMAT = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n",
    "        \"image_name\": tf.io.FixedLenFeature([], tf.string),  # shape [] means single element\n",
    "        # class is missing, this competitions's challenge is to predict flower classes for the test dataset\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n",
    "    image = example['image']\n",
    "    idnum = example['image_name']\n",
    "    return image, idnum if return_image_name else 0\n",
    "\n",
    "\n",
    "def load_dataset(filenames, labeled=True, ordered=False):\n",
    "    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n",
    "    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n",
    "\n",
    "    ignore_order = tf.data.Options()\n",
    "    if not ordered:\n",
    "        ignore_order.experimental_deterministic = False # disable order, increase speed\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n",
    "    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n",
    "    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "80624f00-801f-4e68-95aa-c91fccdba00e",
    "_uuid": "4fe1e5d9-b8ee-4091-b148-dd2a389b9323"
   },
   "outputs": [],
   "source": [
    " def decode_image(image_data, augment=False):\n",
    "    image = tf.image.decode_jpeg(image_data, channels=3)\n",
    "    image = tf.cast(image, tf.float32) / 255.0  # convert image to floats in [0, 1] range\n",
    "    if augment:\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        image = tf.image.random_flip_up_down(image)\n",
    "        image = tf.image.random_saturation(image, 0, 2)\n",
    "        image = tf.image.rot90(image)\n",
    "    image = tf.reshape(image, [IMG_SIZE,IMG_SIZE, 3]) # explicit size needed for TPU\n",
    "    return image\n",
    "\n",
    "def get_training_dataset():\n",
    "    dataset = tf.data.TFRecordDataset(train_filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.repeat() \n",
    "    dataset = dataset.shuffle(1024*8)\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=True), imgname_or_label), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_val_dataset():\n",
    "    dataset = tf.data.TFRecordDataset(valid_filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def get_test_dataset(ordered=False):\n",
    "    dataset = tf.data.TFRecordDataset(test_filenames, num_parallel_reads=AUTO)\n",
    "    dataset = dataset.repeat() \n",
    "    dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, False), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=False), imgname_or_label), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(BATCH_SIZE * REPLICAS)\n",
    "    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n",
    "    return dataset\n",
    "\n",
    "def count_data_items(filenames):\n",
    "    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n",
    "    return np.sum(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "4888c185-915b-47ca-8e6c-b829d906f195",
    "_uuid": "b4f863c4-edd9-4899-8e30-dcea479a9047"
   },
   "outputs": [],
   "source": [
    "def get_lr_callback(batch_size=8):\n",
    "    lr_start   = 0.000005\n",
    "    lr_max     = 0.00000125 * REPLICAS * batch_size\n",
    "    lr_min     = 0.000001\n",
    "    lr_ramp_ep = 5\n",
    "    lr_sus_ep  = 0\n",
    "    lr_decay   = 0.8\n",
    "   \n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_ramp_ep:\n",
    "            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "            \n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep:\n",
    "            lr = lr_max\n",
    "            \n",
    "        else:\n",
    "            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "            \n",
    "        return lr\n",
    "\n",
    "    lr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=False)\n",
    "    return lr_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "0899616c-ea13-41a0-a5a4-b9d6f1489081",
    "_uuid": "4cc14ba6-5475-4ea7-a00e-a01a00817b84"
   },
   "outputs": [],
   "source": [
    "def build_model(dim=512):\n",
    "    inp = tf.keras.layers.Input(shape=(dim,dim,3))\n",
    "    base = efn.EfficientNetB3(input_shape=(dim,dim,3),weights='imagenet',include_top=False)\n",
    "    x = base(inp)\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n",
    "    model = tf.keras.Model(inputs=inp,outputs=x)\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05) \n",
    "    model.compile(optimizer=opt,loss=loss,metrics=['AUC'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "4ecd03b6-1aa9-435b-abf6-2c59b5b1ff77",
    "_uuid": "2f69323b-5a65-405d-9f02-e18f3ef62802"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.1 is available.\r\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install yapl==0.1.2 efficientnet > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "33d5424f-73cf-4066-9cf5-64ebf34281d7",
    "_uuid": "a9a2ae1d-94a7-4bb1-af93-be6422e4fbc7"
   },
   "outputs": [],
   "source": [
    "#model 3\n",
    "import efficientnet.tfkeras as efn\n",
    "input_shape = (512, 512, 3)\n",
    "def create_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        efn.EfficientNetB4(\n",
    "                        input_shape=input_shape,\n",
    "                        weights='imagenet',\n",
    "                        include_top=False\n",
    "                    ),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "ab030cef-84cd-4f46-8efd-e6d38d42213a",
    "_uuid": "39ae51cf-2501-4578-9108-27e1ddd38474"
   },
   "outputs": [],
   "source": [
    "# with strategy.scope():\n",
    "#     model = create_model()\n",
    "# optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "1e06d619-c225-456e-87ac-44e8760ea3a3",
    "_uuid": "dd0a7473-5ccc-4928-9275-4cb2cf2d7a12"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "callbacks = [get_lr_callback(BATCH_SIZE)] \n",
    "step_per_epoch = count_data_items(train_filenames)/BATCH_SIZE//REPLICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "521239d2-bfaa-4cc4-ad82-1ef8687feb9a",
    "_uuid": "62b8191e-9cea-43f6-8b36-dcaba3e23a2d"
   },
   "outputs": [],
   "source": [
    "# warmup_history = model.fit(get_training_dataset(), \n",
    "#                            steps_per_epoch=step_per_epoch, \n",
    "#                            validation_data=get_val_dataset(),\n",
    "#                             epochs=1, \n",
    "#                             verbose=1, callbacks=callbacks).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "e46d17ad-7b28-4710-8449-295004ec1482",
    "_uuid": "462d0bb3-593d-47a8-8eda-a4d0e028c1dd"
   },
   "outputs": [],
   "source": [
    "test_dataset = get_test_dataset(ordered=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "6e2d0700-7802-4968-a410-54ae7202f53a",
    "_uuid": "d6d36085-fb79-4c4c-ab89-4acbf957312c"
   },
   "outputs": [],
   "source": [
    "# test_images_ds = test_dataset.map(lambda image, idnum: image)\n",
    "# ct_test = count_data_items(test_filenames)\n",
    "# STEPS = ct_test/BATCH_SIZE/4/REPLICAS\n",
    "# probabilities = model.predict(test_images_ds, steps=STEPS).flatten()\n",
    "# print(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "b432a48b-16b9-4d32-8300-2b9b2e488470",
    "_uuid": "bedb4f59-b091-481f-87f0-f8604f20ca85"
   },
   "outputs": [],
   "source": [
    "# test_imgs = test_dataset.map(lambda images, ids: images)\n",
    "# img_ids_ds = test_dataset.map(lambda images, ids: ids).unbatch()\n",
    "\n",
    "# img_ids = []\n",
    "# for coutner, ids in enumerate(img_ids_ds):\n",
    "#     if coutner%500 == 0:\n",
    "#         print(coutner)\n",
    "#     img_ids.append(ids.numpy())\n",
    "\n",
    "# img_ids = np.array(img_ids).astype('U')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "3304f6b3-779b-48ef-ae4e-7d43618c2c61",
    "_uuid": "e11592d2-3966-4c01-bf08-943c151f3388"
   },
   "outputs": [],
   "source": [
    "# sample_sub = sample_sub.set_index(\"image_name\").transpose().reindex(columns=list(img_ids)).transpose()\n",
    "# sample_sub[\"target\"] = probabilities\n",
    "# sample_sub.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "5cd296f5-a363-43e5-b23f-feb94c59ee43",
    "_uuid": "69c5cff9-868b-45a8-9fea-a915358826a3"
   },
   "outputs": [],
   "source": [
    "def get_dataset(filename, shuffle=False, repeat=False, return_img_name=False, augment=False, labeled=False, batch_size=32, dim=512):\n",
    "    dataset = tf.data.TFRecordDataset(filename, num_parallel_reads=AUTO)\n",
    "    if repeat:\n",
    "        dataset = dataset.repeat()\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(1024*8)\n",
    "        opt = tf.data.Options()\n",
    "        opt.experimental_deterministic = False\n",
    "        dataset = dataset.with_options(opt)\n",
    "    if labeled:\n",
    "        dataset = dataset.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n",
    "    else:\n",
    "         dataset = dataset.map(lambda example: read_unlabeled_tfrecord(example, return_img_name), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.map(lambda img, imgname_or_label: (decode_image(img, augment=augment), imgname_or_label), num_parallel_calls=AUTO)\n",
    "    dataset = dataset.batch(batch_size * REPLICAS)\n",
    "    dataset = dataset.prefetch(AUTO)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "26c7d397-4491-4240-bdce-7910d87d977f",
    "_uuid": "227a2015-8a2b-47d3-9b6d-1820dd096a68"
   },
   "outputs": [],
   "source": [
    "FOLDS = 5\n",
    "IMG_SIZE = 512\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "weights = 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kfold = KFold(n_splits=FOLDS, shuffle=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b4_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
      "71892992/71892840 [==============================] - 3s 0us/step\n",
      "Epoch 1/12\n",
      "181/181 [==============================] - 171s 946ms/step - accuracy: 0.8308 - loss: 0.5237 - val_accuracy: 0.9260 - val_loss: 0.4903 - lr: 5.0000e-06\n",
      "Epoch 2/12\n",
      "181/181 [==============================] - 146s 807ms/step - accuracy: 0.9224 - loss: 0.2115 - val_accuracy: 0.9752 - val_loss: 0.0905 - lr: 6.8000e-05\n",
      "Epoch 3/12\n",
      "181/181 [==============================] - 144s 795ms/step - accuracy: 0.9330 - loss: 0.1781 - val_accuracy: 0.9502 - val_loss: 0.1245 - lr: 1.3100e-04\n",
      "Epoch 4/12\n",
      "181/181 [==============================] - 144s 794ms/step - accuracy: 0.9380 - loss: 0.1608 - val_accuracy: 0.9216 - val_loss: 0.1803 - lr: 1.9400e-04\n",
      "Epoch 5/12\n",
      "181/181 [==============================] - 144s 793ms/step - accuracy: 0.9434 - loss: 0.1466 - val_accuracy: 0.9032 - val_loss: 0.2439 - lr: 2.5700e-04\n",
      "Epoch 6/12\n",
      "181/181 [==============================] - 144s 794ms/step - accuracy: 0.9500 - loss: 0.1324 - val_accuracy: 0.9273 - val_loss: 0.1821 - lr: 3.2000e-04\n",
      "Epoch 7/12\n",
      "181/181 [==============================] - 144s 794ms/step - accuracy: 0.9573 - loss: 0.1117 - val_accuracy: 0.9108 - val_loss: 0.2649 - lr: 2.5620e-04\n",
      "Epoch 8/12\n",
      "181/181 [==============================] - 144s 795ms/step - accuracy: 0.9651 - loss: 0.0926 - val_accuracy: 0.9222 - val_loss: 0.2743 - lr: 2.0516e-04\n",
      "Epoch 9/12\n",
      "181/181 [==============================] - 144s 795ms/step - accuracy: 0.9722 - loss: 0.0736 - val_accuracy: 0.9074 - val_loss: 0.3693 - lr: 1.6433e-04\n",
      "Epoch 10/12\n",
      "181/181 [==============================] - 144s 794ms/step - accuracy: 0.9764 - loss: 0.0623 - val_accuracy: 0.9027 - val_loss: 0.4074 - lr: 1.3166e-04\n",
      "Epoch 11/12\n",
      "181/181 [==============================] - 144s 795ms/step - accuracy: 0.9818 - loss: 0.0492 - val_accuracy: 0.9102 - val_loss: 0.3957 - lr: 1.0553e-04\n",
      "Epoch 12/12\n",
      "181/181 [==============================] - 144s 794ms/step - accuracy: 0.9856 - loss: 0.0408 - val_accuracy: 0.9087 - val_loss: 0.4559 - lr: 8.4624e-05\n",
      "118/117 [==============================] - 87s 739ms/step\n",
      "1\n",
      "Epoch 1/12\n",
      "191/191 [==============================] - 168s 881ms/step - accuracy: 0.7661 - loss: 0.5435 - val_accuracy: 0.9984 - val_loss: 0.3966 - lr: 5.0000e-06\n",
      "Epoch 2/12\n",
      "191/191 [==============================] - 152s 797ms/step - accuracy: 0.9283 - loss: 0.1989 - val_accuracy: 0.9522 - val_loss: 0.1548 - lr: 6.8000e-05\n",
      "Epoch 3/12\n",
      "191/191 [==============================] - 150s 783ms/step - accuracy: 0.9385 - loss: 0.1653 - val_accuracy: 0.9396 - val_loss: 0.1751 - lr: 1.3100e-04\n",
      "Epoch 4/12\n",
      "191/191 [==============================] - 149s 782ms/step - accuracy: 0.9437 - loss: 0.1510 - val_accuracy: 0.8819 - val_loss: 0.2554 - lr: 1.9400e-04\n",
      "Epoch 5/12\n",
      "191/191 [==============================] - 150s 783ms/step - accuracy: 0.9485 - loss: 0.1375 - val_accuracy: 0.8057 - val_loss: 0.5062 - lr: 2.5700e-04\n",
      "Epoch 6/12\n",
      "191/191 [==============================] - 150s 783ms/step - accuracy: 0.9519 - loss: 0.1256 - val_accuracy: 0.9128 - val_loss: 0.2434 - lr: 3.2000e-04\n",
      "Epoch 7/12\n",
      "191/191 [==============================] - 150s 783ms/step - accuracy: 0.9591 - loss: 0.1081 - val_accuracy: 0.8365 - val_loss: 0.5200 - lr: 2.5620e-04\n",
      "Epoch 8/12\n",
      "191/191 [==============================] - 151s 789ms/step - accuracy: 0.9662 - loss: 0.0896 - val_accuracy: 0.8742 - val_loss: 0.4347 - lr: 2.0516e-04\n",
      "Epoch 9/12\n",
      "191/191 [==============================] - 150s 784ms/step - accuracy: 0.9728 - loss: 0.0728 - val_accuracy: 0.8515 - val_loss: 0.6070 - lr: 1.6433e-04\n",
      "Epoch 10/12\n",
      "191/191 [==============================] - 150s 785ms/step - accuracy: 0.9775 - loss: 0.0601 - val_accuracy: 0.8653 - val_loss: 0.5939 - lr: 1.3166e-04\n",
      "Epoch 11/12\n",
      "191/191 [==============================] - 150s 787ms/step - accuracy: 0.9820 - loss: 0.0477 - val_accuracy: 0.8703 - val_loss: 0.6399 - lr: 1.0553e-04\n",
      "Epoch 12/12\n",
      "191/191 [==============================] - 150s 786ms/step - accuracy: 0.9846 - loss: 0.0423 - val_accuracy: 0.8756 - val_loss: 0.5790 - lr: 8.4624e-05\n",
      "118/117 [==============================] - 85s 724ms/step\n",
      "2\n",
      "Epoch 1/12\n",
      "176/176 [==============================] - 167s 948ms/step - accuracy: 0.7459 - loss: 0.5689 - val_accuracy: 0.9500 - val_loss: 0.4966 - lr: 5.0000e-06\n",
      "Epoch 2/12\n",
      "176/176 [==============================] - 144s 819ms/step - accuracy: 0.9161 - loss: 0.2232 - val_accuracy: 0.9707 - val_loss: 0.0984 - lr: 6.8000e-05\n",
      "Epoch 3/12\n",
      "176/176 [==============================] - 142s 806ms/step - accuracy: 0.9292 - loss: 0.1851 - val_accuracy: 0.9259 - val_loss: 0.1809 - lr: 1.3100e-04\n",
      "Epoch 4/12\n",
      "176/176 [==============================] - 144s 819ms/step - accuracy: 0.9362 - loss: 0.1664 - val_accuracy: 0.9709 - val_loss: 0.0960 - lr: 1.9400e-04\n",
      "Epoch 5/12\n",
      "176/176 [==============================] - 141s 803ms/step - accuracy: 0.9402 - loss: 0.1543 - val_accuracy: 0.9256 - val_loss: 0.2106 - lr: 2.5700e-04\n",
      "Epoch 6/12\n",
      "176/176 [==============================] - 142s 805ms/step - accuracy: 0.9467 - loss: 0.1395 - val_accuracy: 0.9575 - val_loss: 0.1270 - lr: 3.2000e-04\n",
      "Epoch 7/12\n",
      "176/176 [==============================] - 142s 805ms/step - accuracy: 0.9553 - loss: 0.1173 - val_accuracy: 0.9278 - val_loss: 0.2561 - lr: 2.5620e-04\n",
      "Epoch 8/12\n",
      "176/176 [==============================] - 142s 805ms/step - accuracy: 0.9630 - loss: 0.0975 - val_accuracy: 0.9374 - val_loss: 0.2219 - lr: 2.0516e-04\n",
      "Epoch 9/12\n",
      "176/176 [==============================] - 142s 807ms/step - accuracy: 0.9708 - loss: 0.0794 - val_accuracy: 0.9307 - val_loss: 0.2803 - lr: 1.6433e-04\n",
      "Epoch 10/12\n",
      "176/176 [==============================] - 142s 805ms/step - accuracy: 0.9751 - loss: 0.0666 - val_accuracy: 0.9261 - val_loss: 0.3110 - lr: 1.3166e-04\n",
      "Epoch 11/12\n",
      "176/176 [==============================] - 142s 806ms/step - accuracy: 0.9812 - loss: 0.0524 - val_accuracy: 0.9175 - val_loss: 0.3711 - lr: 1.0553e-04\n",
      "Epoch 12/12\n",
      "176/176 [==============================] - 142s 808ms/step - accuracy: 0.9832 - loss: 0.0446 - val_accuracy: 0.9318 - val_loss: 0.3231 - lr: 8.4624e-05\n",
      "118/117 [==============================] - 89s 753ms/step\n",
      "3\n",
      "ok\n",
      "Epoch 1/12\n",
      "175/175 [==============================] - 169s 966ms/step - accuracy: 0.8139 - loss: 0.5405 - val_accuracy: 0.6568 - val_loss: 0.6216 - lr: 5.0000e-06\n",
      "Epoch 2/12\n",
      "175/175 [==============================] - 143s 820ms/step - accuracy: 0.9177 - loss: 0.2241 - val_accuracy: 0.9725 - val_loss: 0.1022 - lr: 6.8000e-05\n",
      "Epoch 3/12\n",
      "175/175 [==============================] - 141s 806ms/step - accuracy: 0.9298 - loss: 0.1829 - val_accuracy: 0.9624 - val_loss: 0.1045 - lr: 1.3100e-04\n",
      "Epoch 4/12\n",
      "175/175 [==============================] - 141s 806ms/step - accuracy: 0.9355 - loss: 0.1678 - val_accuracy: 0.9494 - val_loss: 0.1201 - lr: 1.9400e-04\n",
      "Epoch 5/12\n",
      "175/175 [==============================] - 141s 805ms/step - accuracy: 0.9409 - loss: 0.1538 - val_accuracy: 0.9542 - val_loss: 0.1178 - lr: 2.5700e-04\n",
      "Epoch 6/12\n",
      "175/175 [==============================] - 141s 807ms/step - accuracy: 0.9454 - loss: 0.1417 - val_accuracy: 0.9373 - val_loss: 0.1774 - lr: 3.2000e-04\n",
      "Epoch 7/12\n",
      "175/175 [==============================] - 142s 813ms/step - accuracy: 0.9548 - loss: 0.1172 - val_accuracy: 0.9387 - val_loss: 0.1774 - lr: 2.5620e-04\n",
      "Epoch 8/12\n",
      "175/175 [==============================] - 141s 807ms/step - accuracy: 0.9621 - loss: 0.0975 - val_accuracy: 0.9399 - val_loss: 0.2055 - lr: 2.0516e-04\n",
      "Epoch 9/12\n",
      "175/175 [==============================] - 141s 806ms/step - accuracy: 0.9702 - loss: 0.0785 - val_accuracy: 0.9420 - val_loss: 0.2605 - lr: 1.6433e-04\n",
      "Epoch 10/12\n",
      "175/175 [==============================] - 141s 808ms/step - accuracy: 0.9749 - loss: 0.0667 - val_accuracy: 0.9359 - val_loss: 0.2801 - lr: 1.3166e-04\n",
      "Epoch 11/12\n",
      "175/175 [==============================] - 141s 805ms/step - accuracy: 0.9792 - loss: 0.0554 - val_accuracy: 0.9350 - val_loss: 0.3002 - lr: 1.0553e-04\n",
      "Epoch 12/12\n",
      "175/175 [==============================] - 141s 808ms/step - accuracy: 0.9836 - loss: 0.0447 - val_accuracy: 0.9425 - val_loss: 0.2772 - lr: 8.4624e-05\n",
      "whatup\n",
      "hello\n",
      "118/117 [==============================] - 84s 714ms/step\n",
      "4\n",
      "Epoch 1/12\n",
      "181/181 [==============================] - 170s 940ms/step - accuracy: 0.8284 - loss: 0.5262 - val_accuracy: 0.9030 - val_loss: 0.5405 - lr: 5.0000e-06\n",
      "Epoch 2/12\n",
      "181/181 [==============================] - 148s 817ms/step - accuracy: 0.9213 - loss: 0.2128 - val_accuracy: 0.9738 - val_loss: 0.1044 - lr: 6.8000e-05\n",
      "Epoch 3/12\n",
      "181/181 [==============================] - 145s 801ms/step - accuracy: 0.9299 - loss: 0.1819 - val_accuracy: 0.9587 - val_loss: 0.1079 - lr: 1.3100e-04\n",
      "Epoch 4/12\n",
      "181/181 [==============================] - 145s 801ms/step - accuracy: 0.9370 - loss: 0.1629 - val_accuracy: 0.9340 - val_loss: 0.1562 - lr: 1.9400e-04\n",
      "Epoch 5/12\n",
      "181/181 [==============================] - 145s 801ms/step - accuracy: 0.9424 - loss: 0.1506 - val_accuracy: 0.9391 - val_loss: 0.1566 - lr: 2.5700e-04\n",
      "Epoch 6/12\n",
      "181/181 [==============================] - 145s 802ms/step - accuracy: 0.9481 - loss: 0.1377 - val_accuracy: 0.9259 - val_loss: 0.2320 - lr: 3.2000e-04\n",
      "Epoch 7/12\n",
      "181/181 [==============================] - 145s 800ms/step - accuracy: 0.9556 - loss: 0.1158 - val_accuracy: 0.9282 - val_loss: 0.2277 - lr: 2.5620e-04\n",
      "Epoch 8/12\n",
      "181/181 [==============================] - 145s 802ms/step - accuracy: 0.9655 - loss: 0.0919 - val_accuracy: 0.9060 - val_loss: 0.3096 - lr: 2.0516e-04\n",
      "Epoch 9/12\n",
      "181/181 [==============================] - 145s 801ms/step - accuracy: 0.9726 - loss: 0.0756 - val_accuracy: 0.9046 - val_loss: 0.3826 - lr: 1.6433e-04\n",
      "Epoch 10/12\n",
      "181/181 [==============================] - 145s 799ms/step - accuracy: 0.9766 - loss: 0.0633 - val_accuracy: 0.9121 - val_loss: 0.3402 - lr: 1.3166e-04\n",
      "Epoch 11/12\n",
      "181/181 [==============================] - 145s 802ms/step - accuracy: 0.9812 - loss: 0.0509 - val_accuracy: 0.9163 - val_loss: 0.4044 - lr: 1.0553e-04\n",
      "Epoch 12/12\n",
      "181/181 [==============================] - 145s 801ms/step - accuracy: 0.9847 - loss: 0.0421 - val_accuracy: 0.9085 - val_loss: 0.4342 - lr: 8.4624e-05\n",
      "118/117 [==============================] - 84s 716ms/step\n"
     ]
    }
   ],
   "source": [
    "TTA = 11\n",
    "preds = np.zeros((count_data_items(test_filenames),1))\n",
    "for fold, (train_file, valid_file) in enumerate(kfold.split(data_filenames)):\n",
    "    print(fold)\n",
    "    training_dataset = get_dataset([data_filenames[x] for x in train_file], augment=True, shuffle=True, repeat=True, labeled=True)\n",
    "    valid_dataset = get_dataset([data_filenames[x] for x in valid_file], augment=True, shuffle=False, repeat=False, labeled=False)\n",
    "    K.clear_session()\n",
    "    with strategy.scope():\n",
    "        model = create_model()\n",
    "    if fold == 3:\n",
    "        print(\"ok\")\n",
    "    optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    callbacks = [get_lr_callback(BATCH_SIZE)] \n",
    "    steps_per_epoch = count_data_items([data_filenames[x] for x in train_file])/BATCH_SIZE//REPLICAS\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint('fold-%i.h5'%fold, monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min', save_freq='epoch')\n",
    "    callbacks.append(sv)\n",
    "    warmup_history = model.fit(training_dataset, steps_per_epoch=steps_per_epoch, validation_data=valid_dataset, epochs=12, verbose=1, callbacks=callbacks).history\n",
    "    if fold == 3:\n",
    "        print(\"whatup\")\n",
    "\n",
    "\n",
    "    test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n",
    "    ct_valid = count_data_items(test_filenames)\n",
    "    if fold == 3:\n",
    "        print(\"hello\")\n",
    "    STEPS = TTA * ct_valid/32/4/REPLICAS\n",
    "    pred = model.predict(test_dataset,steps=STEPS,verbose=1)[:TTA*ct_valid,]\n",
    "    preds[:, 0] += np.mean(pred.reshape((ct_valid,TTA),order='F'),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = preds/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_dataset(test_filenames, augment=False, repeat=False,\n",
    "                 labeled=False, return_img_name=True)\n",
    "\n",
    "image_names = np.array([img_name.numpy().decode(\"utf-8\") \n",
    "                        for img, img_name in iter(ds.unbatch())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_name</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ISIC_0052060</th>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0052349</th>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0058510</th>\n",
       "      <td>0.000021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0073313</th>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ISIC_0073502</th>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                target\n",
       "image_name            \n",
       "ISIC_0052060  0.000007\n",
       "ISIC_0052349  0.000007\n",
       "ISIC_0058510  0.000021\n",
       "ISIC_0073313  0.000073\n",
       "ISIC_0073502  0.000007"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.DataFrame(dict(image_name=list(image_names), target=preds))\n",
    "submission = submission.sort_values('image_name')\n",
    "submission = submission.set_index(\"image_name\")\n",
    "submission.to_csv('submission.csv', index=True)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "# train_datagen = ImageDataGenerator(\n",
    "#         shear_range=0.1,\n",
    "#         zoom_range=0.1,\n",
    "#         horizontal_flip=True,\n",
    "#         rotation_range=10.,\n",
    "#         fill_mode='reflect',\n",
    "#         width_shift_range = 0.1, \n",
    "#         height_shift_range = 0.1)\n",
    "# train_datagen.flow(get_dataset(train_filenames, augment=False, shuffle=False, repeat=True, labeled=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# train_file, valid_file = train_test_split(train_filenames, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataset = get_dataset(train_file, augment=True, shuffle=True, repeat=True, labeled=True)\n",
    "# valid_dataset = get_dataset(valid_file, augment=True, shuffle=False, repeat=True, labeled=False, batch_size=32*5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA = 10\n",
    "# oof_pred = []\n",
    "# test_dataset = get_dataset(test_filenames, augment=True, repeat=True, batch_size=BATCH_SIZE*4)\n",
    "# ct_valid = count_data_items(test_filenames)\n",
    "# STEPS = TTA * ct_valid/32/4/REPLICAS\n",
    "# pred = model.predict(test_dataset,steps=STEPS,verbose=1)       \n",
    "# preds = pred[:TTA*ct_valid,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof_pred.append( np.mean(preds.reshape((ct_valid,TTA),order='F'),axis=1) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((count_data_items(test_filenames),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10982"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_sub)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
